<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>S·ªï Tay L·ªánh Th·ª±c Chi·∫øn Hadoop & Linux</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        /* CSS C∆° b·∫£n */
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; padding: 20px; }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        h2 { color: #2980b9; margin-top: 30px; }
        hr { border: 0; border-top: 1px solid #ecf0f1; margin: 20px 0; }
        p { margin-bottom: 15px; }
        
        /* Style cho b·∫£ng l·ªánh */
        table { width: 100%; border-collapse: collapse; margin-top: 20px; table-layout: fixed; }
        th, td { border: 1px solid #bdc3c7; padding: 8px; text-align: left; word-wrap: break-word; font-size: 14px; }
        th { background-color: #3498db; color: white; font-weight: bold; }
        tr:nth-child(even) { background-color: #f8f9fa; }
        .command-group { background-color: #2980b9 !important; color: white !important; font-weight: bold; text-align: center; }
        
        /* Style cho code */
        code { background-color: #eee; padding: 2px 4px; border-radius: 3px; font-family: monospace; font-size: 13px; }
    </style>
</head>
<body>

<h1>üìú S·ªï Tay L·ªánh Th·ª±c Chi·∫øn Hadoop 3.3.6 (WSL2)</h1>
<p>B·∫£ng t·ªïng h·ª£p ƒë·∫ßy ƒë·ªß c√°c l·ªánh ƒë√£ s·ª≠ d·ª•ng ƒë·ªÉ c√†i ƒë·∫∑t, c·∫•u h√¨nh, qu·∫£n l√Ω v√† ch·∫°y Job WordCount th√†nh c√¥ng. ƒê√¢y l√† t√†i li·ªáu quan tr·ªçng nh·∫•t cho giai ƒëo·∫°n h·ªçc PySpark s·∫Øp t·ªõi.</p>

<hr>

<h2>I. L·ªánh Linux C·ª•c B·ªô (Thao t√°c Chu·∫©n b·ªã)</h2>

<table>
    <thead>
        <tr>
            <th>L·ªánh</th>
            <th>M·ª•c ƒë√≠ch & √ù nghƒ©a</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>mkdir ~/hdfs_training</code></td>
            <td>**T·∫°o Th∆∞ m·ª•c:** T·∫°o th∆∞ m·ª•c l√†m vi·ªác m·ªõi tr√™n ·ªï ƒëƒ©a Linux.</td>
        </tr>
        <tr>
            <td><code>cd ~/hdfs_training</code></td>
            <td>**Di chuy·ªÉn:** Thay ƒë·ªïi th∆∞ m·ª•c hi·ªán t·∫°i.</td>
        </tr>
        <tr>
            <td><code>ls -lh server_logs.txt</code></td>
            <td>**Li·ªát k√™ chi ti·∫øt:** Xem file v·ªõi k√≠ch th∆∞·ªõc d·ªÖ ƒë·ªçc (Human-readable).</td>
        </tr>
        <tr>
            <td><code>echo "..." >> server_logs.txt</code></td>
            <td>**Ghi File:** Th√™m n·ªôi dung v√†o cu·ªëi file text.</td>
        </tr>
        <tr>
            <td><code>FILE_NAME=$(ls -1 | head -n 1)</code></td>
            <td>**G√°n Bi·∫øn T·ª± ƒë·ªông:** L·∫•y t√™n file ƒë·∫ßu ti√™n trong th∆∞ m·ª•c v√† g√°n v√†o bi·∫øn <code>FILE_NAME</code>.</td>
        </tr>
    </tbody>
</table>

<h2>II. L·ªánh HDFS (Thao t√°c L∆∞u tr·ªØ tr√™n C·ª•m Ph√¢n t√°n)</h2>

<table>
    <thead>
        <tr>
            <th>L·ªánh</th>
            <th>M·ª•c ƒë√≠ch & √ù nghƒ©a</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>hdfs dfs -mkdir -p /projects/bigdata_lab/raw_data</code></td>
            <td>T·∫°o th∆∞ m·ª•c ph√¢n c·∫•p tr√™n HDFS.</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -put server_logs.txt /user/$(whoami)/input</code></td>
            <td>**ƒê·∫©y File l√™n (Upload):** Chuy·ªÉn file t·ª´ Linux c·ª•c b·ªô l√™n HDFS.</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -get /user/$(whoami)/output_4/part-r-00000 ./result.txt</code></td>
            <td>**K√©o File xu·ªëng (Download):** T·∫£i file t·ª´ HDFS v·ªÅ m√°y c·ª•c b·ªô.</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -ls /user/$(whoami)/output_grep</code></td>
            <td>Li·ªát k√™ n·ªôi dung th∆∞ m·ª•c tr√™n HDFS.</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -rm -r /user/$(whoami)/output_4</code></td>
            <td>**X√≥a Th∆∞ m·ª•c (ƒê·ªá quy):** X√≥a th∆∞ m·ª•c c√πng v·ªõi c√°c file b√™n trong (c·∫ßn c·∫©n th·∫≠n).</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -cat /user/$(whoami)/output_grep/part-r-00000</code></td>
            <td>Xem n·ªôi dung c·ªßa file tr√™n HDFS (in ra terminal).</td>
        </tr>
        <tr>
            <td><code>hdfs dfs -stat %r /user/$(whoami)/input/file.txt</code></td>
            <td>Ki·ªÉm tra **Replication Factor** (s·ªë b·∫£n sao) c·ªßa file.</td>
        </tr>
    </tbody>
</table>

<h2>III. L·ªánh H·ªá th·ªëng v√† ·ª®ng d·ª•ng (YARN & Kh·ªüi ƒë·ªông)</h2>

<table>
    <thead>
        <tr>
            <th>L·ªánh</th>
            <th>M·ª•c ƒë√≠ch & √ù nghƒ©a</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><code>stop-all.sh</code></td>
            <td>D·ª´ng ho·∫°t ƒë·ªông t·∫•t c·∫£ c√°c d·ªãch v·ª• Hadoop (HDFS v√† YARN).</td>
        </tr>
        <tr>
            <td><code>start-all.sh</code></td>
            <td>Ch·∫°y to√†n b·ªô h·ªá th·ªëng c·ª•m Hadoop.</td>
        </tr>
        <tr>
            <td><code>jps</code></td>
            <td>Ki·ªÉm tra c√°c ti·∫øn tr√¨nh Java ƒëang ch·∫°y (X√°c nh·∫≠n 5 Daemon c·ªßa Hadoop).</td>
        </tr>
        <tr>
            <td><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep
                 /user/$(whoami)/input_logs /user/$(whoami)/output_grep ERROR</code></td>
            <td>Ch·∫°y **Job Grep** (B·ªô l·ªçc) tr√™n YARN ƒë·ªÉ t√¨m chu·ªói <code>ERROR</code>.</td>
        </tr>
        <tr>
            <td><code>hadoop jar ... wordcount /input /output_4</code></td>
            <td>Ch·∫°y Job WordCount (ƒê·∫øm t·ª´).</td>
        </tr>
    </tbody>
</table>

</body>
</html>